{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea616e0",
   "metadata": {
    "papermill": {
     "duration": 0.0065,
     "end_time": "2025-07-26T13:26:56.368160",
     "exception": false,
     "start_time": "2025-07-26T13:26:56.361660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **IMPORTING LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5789e115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:26:56.380618Z",
     "iopub.status.busy": "2025-07-26T13:26:56.379926Z",
     "iopub.status.idle": "2025-07-26T13:27:07.564180Z",
     "shell.execute_reply": "2025-07-26T13:27:07.563559Z"
    },
    "papermill": {
     "duration": 11.191892,
     "end_time": "2025-07-26T13:27:07.565580",
     "exception": false,
     "start_time": "2025-07-26T13:26:56.373688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, jaccard_score\n",
    "from functools import partial\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5584f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.577820Z",
     "iopub.status.busy": "2025-07-26T13:27:07.577458Z",
     "iopub.status.idle": "2025-07-26T13:27:07.581415Z",
     "shell.execute_reply": "2025-07-26T13:27:07.580881Z"
    },
    "papermill": {
     "duration": 0.011095,
     "end_time": "2025-07-26T13:27:07.582528",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.571433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add at the top of the file after imports\n",
    "torch.backends.cuda.max_memory_reserved = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Add these environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13dbcd96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.594604Z",
     "iopub.status.busy": "2025-07-26T13:27:07.594385Z",
     "iopub.status.idle": "2025-07-26T13:27:07.682705Z",
     "shell.execute_reply": "2025-07-26T13:27:07.681854Z"
    },
    "papermill": {
     "duration": 0.096158,
     "end_time": "2025-07-26T13:27:07.684008",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.587850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUDA Setup ===\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device name: Tesla P100-PCIE-16GB\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA setup\n",
    "def check_cuda():\n",
    "    \"\"\"Check and print CUDA information once\"\"\"\n",
    "    print(\"\\n=== CUDA Setup ===\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"=================\\n\")\n",
    "\n",
    "# Call check_cuda once\n",
    "check_cuda()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bed4d8",
   "metadata": {
    "papermill": {
     "duration": 0.005329,
     "end_time": "2025-07-26T13:27:07.694975",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.689646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **BEiT Backbone Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d943fa00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.707067Z",
     "iopub.status.busy": "2025-07-26T13:27:07.706821Z",
     "iopub.status.idle": "2025-07-26T13:27:07.714548Z",
     "shell.execute_reply": "2025-07-26T13:27:07.713986Z"
    },
    "papermill": {
     "duration": 0.015171,
     "end_time": "2025-07-26T13:27:07.715521",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.700350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add this before creating the datasets\n",
    "import glob\n",
    "\n",
    "# Check the actual directory structure\n",
    "def check_directory_structure(base_dir):\n",
    "    print(f\"\\n--- Directory Structure Check for {base_dir} ---\")\n",
    "    \n",
    "    # Check main directories\n",
    "    train_img_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "    train_mask_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "    val_img_dir = os.path.join(base_dir, \"valid\", \"images\")\n",
    "    val_mask_dir = os.path.join(base_dir, \"valid\", \"labels\")\n",
    "    \n",
    "    for dir_path in [train_img_dir, train_mask_dir, val_img_dir, val_mask_dir]:\n",
    "        exists = os.path.exists(dir_path)\n",
    "        if exists:\n",
    "            files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "            file_count = len(files)\n",
    "            print(f\"Directory {dir_path}: {'Exists' if exists else 'MISSING'}, Contains {file_count} files\")\n",
    "            if file_count > 0:\n",
    "                # Show some example files\n",
    "                print(f\"  Example files: {[os.path.basename(f) for f in files[:3]]}\")\n",
    "        else:\n",
    "            print(f\"Directory {dir_path}: MISSING\")\n",
    "    \n",
    "    # Try to find the first image and its corresponding mask\n",
    "    if os.path.exists(train_img_dir) and os.path.exists(train_mask_dir):\n",
    "        img_files = glob.glob(os.path.join(train_img_dir, \"*.jpg\")) + \\\n",
    "                    glob.glob(os.path.join(train_img_dir, \"*.png\"))\n",
    "        \n",
    "        if img_files:\n",
    "            # Get first image\n",
    "            first_img = img_files[0]\n",
    "            img_basename = os.path.splitext(os.path.basename(first_img))[0]\n",
    "            \n",
    "            # Look for matching mask\n",
    "            potential_masks = [\n",
    "                os.path.join(train_mask_dir, f\"{img_basename}.txt\"),\n",
    "                os.path.join(train_mask_dir, f\"{img_basename}.png\"),\n",
    "                os.path.join(train_mask_dir, f\"{img_basename}.jpg\")\n",
    "            ]\n",
    "            \n",
    "            found_mask = None\n",
    "            for mask_path in potential_masks:\n",
    "                if os.path.exists(mask_path):\n",
    "                    found_mask = mask_path\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\nMatching check:\")\n",
    "            print(f\"  Image: {first_img}\")\n",
    "            print(f\"  Mask: {found_mask if found_mask else 'NO MATCHING MASK'}\")\n",
    "            \n",
    "            if found_mask and found_mask.endswith('.txt'):\n",
    "                # Show the content of the txt mask\n",
    "                with open(found_mask, 'r') as f:\n",
    "                    mask_content = f.read()\n",
    "                print(f\"\\nMask content (first 200 chars):\\n{mask_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983fd90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.727244Z",
     "iopub.status.busy": "2025-07-26T13:27:07.727027Z",
     "iopub.status.idle": "2025-07-26T13:27:07.731547Z",
     "shell.execute_reply": "2025-07-26T13:27:07.730846Z"
    },
    "papermill": {
     "duration": 0.011684,
     "end_time": "2025-07-26T13:27:07.732618",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.720934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37acbdf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.744092Z",
     "iopub.status.busy": "2025-07-26T13:27:07.743869Z",
     "iopub.status.idle": "2025-07-26T13:27:07.747291Z",
     "shell.execute_reply": "2025-07-26T13:27:07.746751Z"
    },
    "papermill": {
     "duration": 0.010416,
     "end_time": "2025-07-26T13:27:07.748389",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.737973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05349809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.759894Z",
     "iopub.status.busy": "2025-07-26T13:27:07.759696Z",
     "iopub.status.idle": "2025-07-26T13:27:07.764749Z",
     "shell.execute_reply": "2025-07-26T13:27:07.764229Z"
    },
    "papermill": {
     "duration": 0.011983,
     "end_time": "2025-07-26T13:27:07.765770",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.753787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated within the interval [a, b].\n",
    "        # (1) Generate values from the uniform distribution U(low, high).\n",
    "        low = norm_cdf((a - mean) / std)\n",
    "        high = norm_cdf((b - mean) / std)\n",
    "        tensor.uniform_(low, high)\n",
    "        # (2) Use the inverse CDF transform for the normal distribution.\n",
    "        tensor.erfinv_()\n",
    "        # (3) Transform to the desired mean and std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "        # (4) Clamp to ensure values are still in the interval [a, b]\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586f5967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.777064Z",
     "iopub.status.busy": "2025-07-26T13:27:07.776838Z",
     "iopub.status.idle": "2025-07-26T13:27:07.782364Z",
     "shell.execute_reply": "2025-07-26T13:27:07.781783Z"
    },
    "papermill": {
     "duration": 0.012359,
     "end_time": "2025-07-26T13:27:07.783396",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.771037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9def05df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.795037Z",
     "iopub.status.busy": "2025-07-26T13:27:07.794798Z",
     "iopub.status.idle": "2025-07-26T13:27:07.807200Z",
     "shell.execute_reply": "2025-07-26T13:27:07.806664Z"
    },
    "papermill": {
     "duration": 0.019442,
     "end_time": "2025-07-26T13:27:07.808216",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.788774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "            proj_drop=0., window_size=None, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        if window_size:\n",
    "            self.window_size = window_size\n",
    "            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(window_size[0])\n",
    "            coords_w = torch.arange(window_size[1])\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "            relative_position_index = \\\n",
    "                torch.zeros(size=(window_size[0] * window_size[1] + 1, ) * 2, dtype=relative_coords.dtype)\n",
    "            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            relative_position_index[0, 0:] = self.num_relative_distance - 3\n",
    "            relative_position_index[0:, 0] = self.num_relative_distance - 2\n",
    "            relative_position_index[0, 0] = self.num_relative_distance - 1\n",
    "\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        else:\n",
    "            self.window_size = None\n",
    "            self.relative_position_bias_table = None\n",
    "            self.relative_position_index = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, rel_pos_bias=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        if self.relative_position_bias_table is not None:\n",
    "            relative_position_bias = \\\n",
    "                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                    self.window_size[0] * self.window_size[1] + 1,\n",
    "                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if rel_pos_bias is not None:\n",
    "            attn = attn + rel_pos_bias\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716e73cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.819711Z",
     "iopub.status.busy": "2025-07-26T13:27:07.819508Z",
     "iopub.status.idle": "2025-07-26T13:27:07.826167Z",
     "shell.execute_reply": "2025-07-26T13:27:07.825485Z"
    },
    "papermill": {
     "duration": 0.013779,
     "end_time": "2025-07-26T13:27:07.827327",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.813548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, rel_pos_bias=None):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48921d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.838750Z",
     "iopub.status.busy": "2025-07-26T13:27:07.838545Z",
     "iopub.status.idle": "2025-07-26T13:27:07.843876Z",
     "shell.execute_reply": "2025-07-26T13:27:07.843208Z"
    },
    "papermill": {
     "duration": 0.012275,
     "end_time": "2025-07-26T13:27:07.844980",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.832705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x)\n",
    "        Hp, Wp = x.shape[2], x.shape[3]\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x, (Hp, Wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ce8590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.856808Z",
     "iopub.status.busy": "2025-07-26T13:27:07.856611Z",
     "iopub.status.idle": "2025-07-26T13:27:07.863125Z",
     "shell.execute_reply": "2025-07-26T13:27:07.862594Z"
    },
    "papermill": {
     "duration": 0.013869,
     "end_time": "2025-07-26T13:27:07.864201",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.850332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(self, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(window_size[0])\n",
    "        coords_w = torch.arange(window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "        relative_position_index = \\\n",
    "            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n",
    "        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        relative_position_index[0, 0:] = self.num_relative_distance - 3\n",
    "        relative_position_index[0:, 0] = self.num_relative_distance - 2\n",
    "        relative_position_index[0, 0] = self.num_relative_distance - 1\n",
    "\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def forward(self):\n",
    "        relative_position_bias = \\\n",
    "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size[0] * self.window_size[1] + 1,\n",
    "                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f0a2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.876320Z",
     "iopub.status.busy": "2025-07-26T13:27:07.876115Z",
     "iopub.status.idle": "2025-07-26T13:27:07.890627Z",
     "shell.execute_reply": "2025-07-26T13:27:07.890132Z"
    },
    "papermill": {
     "duration": 0.021826,
     "end_time": "2025-07-26T13:27:07.891637",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.869811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BEiT(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=80, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=None, init_values=None, use_checkpoint=False, \n",
    "                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n",
    "                 out_indices=[3, 5, 7, 11]):\n",
    "        super().__init__()\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        if use_abs_pos_emb:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        else:\n",
    "            self.pos_embed = None\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if use_shared_rel_pos_bias:\n",
    "            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n",
    "        else:\n",
    "            self.rel_pos_bias = None\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.use_rel_pos_bias = use_rel_pos_bias\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        if self.pos_embed is not None:\n",
    "            trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "        if patch_size == 16:\n",
    "            self.fpn1 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(embed_dim),\n",
    "                nn.GELU(),\n",
    "                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "            self.fpn2 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "            self.fpn3 = nn.Identity()\n",
    "\n",
    "            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif patch_size == 8:\n",
    "            self.fpn1 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "            self.fpn2 = nn.Identity()\n",
    "\n",
    "            self.fpn3 = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "            self.fpn4 = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            )\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x, (Hp, Wp) = self.patch_embed(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n",
    "        features = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, rel_pos_bias)\n",
    "            else:\n",
    "                x = blk(x, rel_pos_bias)\n",
    "            if i in self.out_indices:\n",
    "                xp = x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n",
    "                features.append(xp.contiguous())\n",
    "\n",
    "        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n",
    "        for i in range(len(features)):\n",
    "            features[i] = ops[i](features[i])\n",
    "\n",
    "        return tuple(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5c450",
   "metadata": {
    "papermill": {
     "duration": 0.0053,
     "end_time": "2025-07-26T13:27:07.902341",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.897041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Segmentation Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8437dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.913786Z",
     "iopub.status.busy": "2025-07-26T13:27:07.913564Z",
     "iopub.status.idle": "2025-07-26T13:27:07.917374Z",
     "shell.execute_reply": "2025-07-26T13:27:07.916828Z"
    },
    "papermill": {
     "duration": 0.010763,
     "end_time": "2025-07-26T13:27:07.918411",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.907648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define SegmentationHead\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aafe2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.930696Z",
     "iopub.status.busy": "2025-07-26T13:27:07.930490Z",
     "iopub.status.idle": "2025-07-26T13:27:07.937238Z",
     "shell.execute_reply": "2025-07-26T13:27:07.936507Z"
    },
    "papermill": {
     "duration": 0.01429,
     "end_time": "2025-07-26T13:27:07.938448",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.924158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the BEiT-based Semantic Segmentation model\n",
    "class BEiTSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Calculate patch dimensions for 256x256 input\n",
    "        img_size = 256  # Match transform size\n",
    "        patch_size = 16\n",
    "        num_patches = (img_size // patch_size) ** 2  # 256/16 = 16 patches per side\n",
    "        \n",
    "        self.backbone = BEiT(\n",
    "            img_size=img_size,       # Changed to match transform size\n",
    "            patch_size=patch_size,\n",
    "            in_chans=3,\n",
    "            embed_dim=512,\n",
    "            depth=6,\n",
    "            num_heads=8,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            use_abs_pos_emb=True,\n",
    "            use_rel_pos_bias=False,\n",
    "            init_values=0.1,\n",
    "            drop_path_rate=0.1,\n",
    "            out_indices=[2, 3, 4, 5],\n",
    "            use_checkpoint=True\n",
    "        )\n",
    "        \n",
    "        # Adjust decoder for 256x256 input\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        \n",
    "        self.segmentation_head = SegmentationHead(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        x = features[-1]\n",
    "        x = self.decoder(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        # Interpolate to input size\n",
    "        x = nn.functional.interpolate(x, size=(256, 256), mode='bilinear', align_corners=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4903a",
   "metadata": {
    "papermill": {
     "duration": 0.005294,
     "end_time": "2025-07-26T13:27:07.949176",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.943882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset and Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4cc8b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.961129Z",
     "iopub.status.busy": "2025-07-26T13:27:07.960595Z",
     "iopub.status.idle": "2025-07-26T13:27:07.964076Z",
     "shell.execute_reply": "2025-07-26T13:27:07.963528Z"
    },
    "papermill": {
     "duration": 0.010636,
     "end_time": "2025-07-26T13:27:07.965209",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.954573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---------------  ---------------#\n",
    "\n",
    "# Load dataset configuration\n",
    "def load_data_config(yaml_path):\n",
    "    \"\"\"Load dataset configuration from YAML file\"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28204a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.977498Z",
     "iopub.status.busy": "2025-07-26T13:27:07.977013Z",
     "iopub.status.idle": "2025-07-26T13:27:07.987237Z",
     "shell.execute_reply": "2025-07-26T13:27:07.986670Z"
    },
    "papermill": {
     "duration": 0.017589,
     "end_time": "2025-07-26T13:27:07.988372",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.970783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Dataset for semantic segmentation\n",
    "class TumorSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, num_classes=3):  # Add num_classes parameter\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes  # Store num_classes\n",
    "        \n",
    "        # Support both extensions\n",
    "        self.image_files = [f for f in os.listdir(image_dir) \n",
    "                          if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        # For each image, find the corresponding mask\n",
    "        self.valid_pairs = []\n",
    "        for img_file in self.image_files:\n",
    "            img_name = os.path.splitext(img_file)[0]\n",
    "            mask_file = None\n",
    "            \n",
    "            # Look for masks with different extensions\n",
    "            for ext in ['.txt', '.png', '.jpg']:\n",
    "                candidate = os.path.join(mask_dir, img_name + ext)\n",
    "                if os.path.exists(candidate):\n",
    "                    mask_file = img_name + ext\n",
    "                    break\n",
    "            \n",
    "            if mask_file:\n",
    "                self.valid_pairs.append((img_file, mask_file))\n",
    "        \n",
    "        print(f\"Found {len(self.valid_pairs)} valid image-mask pairs out of {len(self.image_files)} images\")\n",
    "        \n",
    "        if not self.valid_pairs:\n",
    "            print(\"WARNING: No valid image-mask pairs found!\")\n",
    "            print(\"This will cause an error with DataLoader.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file, mask_file = self.valid_pairs[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "        \n",
    "        # Handle different mask formats\n",
    "        if mask_file.endswith('.txt'):  # YOLO format\n",
    "            h, w = image.shape[:2]\n",
    "            mask = np.zeros((h, w), dtype=np.int64)\n",
    "            \n",
    "            try:\n",
    "                with open(mask_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        # Ensure class_id is valid\n",
    "                        if class_id >= self.num_classes:\n",
    "                            print(f\"Warning: Invalid class ID {class_id} in {mask_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        x_center, y_center, bbox_width, bbox_height = map(float, parts[1:5])\n",
    "                        \n",
    "                        x1 = max(0, int((x_center - bbox_width/2) * w))\n",
    "                        y1 = max(0, int((y_center - bbox_height/2) * h))\n",
    "                        x2 = min(w-1, int((x_center + bbox_width/2) * w))\n",
    "                        y2 = min(h-1, int((y_center + bbox_height/2) * h))\n",
    "                        \n",
    "                        mask[y1:y2+1, x1:x2+1] = class_id  # Remove +1 offset\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing mask {mask_path}: {e}\")\n",
    "                mask = np.zeros((h, w), dtype=np.int64)\n",
    "        else:  # Image format\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = mask.astype(np.int64)\n",
    "            # Ensure mask values are valid class indices\n",
    "            mask[mask >= self.num_classes] = 0\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask'].long()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21df070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:07.999909Z",
     "iopub.status.busy": "2025-07-26T13:27:07.999697Z",
     "iopub.status.idle": "2025-07-26T13:27:08.004476Z",
     "shell.execute_reply": "2025-07-26T13:27:08.003980Z"
    },
    "papermill": {
     "duration": 0.011742,
     "end_time": "2025-07-26T13:27:08.005482",
     "exception": false,
     "start_time": "2025-07-26T13:27:07.993740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc=\"Training\") as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            \n",
    "            # Clear cache every few iterations\n",
    "            if pbar.n % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11cd25c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.016988Z",
     "iopub.status.busy": "2025-07-26T13:27:08.016765Z",
     "iopub.status.idle": "2025-07-26T13:27:08.022501Z",
     "shell.execute_reply": "2025-07-26T13:27:08.022010Z"
    },
    "papermill": {
     "duration": 0.012588,
     "end_time": "2025-07-26T13:27:08.023476",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.010888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader, desc=\"Validating\") as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                targets = masks.cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds.flatten())\n",
    "                all_targets.extend(targets.flatten())\n",
    "                \n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Calculate IoU for each class\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = np.array(all_preds) == cls\n",
    "        target_cls = np.array(all_targets) == cls\n",
    "        intersection = np.logical_and(pred_cls, target_cls).sum()\n",
    "        union = np.logical_or(pred_cls, target_cls).sum()\n",
    "        if union == 0:\n",
    "            iou = float('nan')\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        iou_per_class.append(iou)\n",
    "    mean_iou = np.nanmean(iou_per_class)\n",
    "    return total_loss / len(dataloader), mean_iou, iou_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81c4722d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.035849Z",
     "iopub.status.busy": "2025-07-26T13:27:08.035378Z",
     "iopub.status.idle": "2025-07-26T13:27:08.044670Z",
     "shell.execute_reply": "2025-07-26T13:27:08.043990Z"
    },
    "papermill": {
     "duration": 0.016748,
     "end_time": "2025-07-26T13:27:08.045768",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.029020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_prediction(model, test_img_path, device, class_names):\n",
    "    \"\"\"Visualize prediction on a single test image with improved overlay\"\"\"\n",
    "    # Prepare image\n",
    "    image = cv2.imread(test_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {test_img_path}\")\n",
    "        return\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Store original dimensions\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    \n",
    "    # Resize image to model input size\n",
    "    target_size = (256, 256)\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    \n",
    "    # Apply transforms\n",
    "    transform = A.Compose([\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Transform and add batch dimension\n",
    "    transformed = transform(image=resized_image)\n",
    "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred_mask = torch.argmax(output, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    # Resize prediction mask back to original image size\n",
    "    pred_mask = cv2.resize(pred_mask.astype(float), (orig_w, orig_h), \n",
    "                          interpolation=cv2.INTER_NEAREST).astype(int)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 1. Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 2. Prediction mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    # Create a colored mask\n",
    "    mask_display = np.zeros((*pred_mask.shape, 3), dtype=np.uint8)\n",
    "    tumor_regions = (pred_mask == 1)\n",
    "    mask_display[tumor_regions] = [255, 0, 0]  # Red for tumor\n",
    "    plt.imshow(mask_display)\n",
    "    plt.title('Predicted Segmentation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 3. Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    # Only create overlay if tumor regions are detected\n",
    "    if np.any(tumor_regions):\n",
    "        # Create tumor overlay\n",
    "        mask_overlay = np.zeros_like(image)\n",
    "        mask_overlay[tumor_regions] = [255, 0, 0]\n",
    "        \n",
    "        # Blend images\n",
    "        alpha = 0.5\n",
    "        overlay = cv2.addWeighted(image, 1, mask_overlay, alpha, 0)\n",
    "        \n",
    "        # Add contours\n",
    "        tumor_mask_uint8 = tumor_regions.astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(tumor_mask_uint8, \n",
    "                                     cv2.RETR_EXTERNAL, \n",
    "                                     cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, (255, 255, 255), 2)\n",
    "    \n",
    "    plt.imshow(overlay)\n",
    "    plt.title('Overlay with Tumor Regions' if np.any(tumor_regions) else 'No Tumor Detected')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save with high quality\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'prediction_epoch_{epoch+1}.png', \n",
    "                dpi=300, \n",
    "                bbox_inches='tight',\n",
    "                pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Print detection status\n",
    "    if not np.any(tumor_regions):\n",
    "        print(f\"No tumor regions detected in {os.path.basename(test_img_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b18c3",
   "metadata": {
    "papermill": {
     "duration": 0.005128,
     "end_time": "2025-07-26T13:27:08.056181",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.051053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80fb8426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.067532Z",
     "iopub.status.busy": "2025-07-26T13:27:08.067325Z",
     "iopub.status.idle": "2025-07-26T13:27:08.084069Z",
     "shell.execute_reply": "2025-07-26T13:27:08.083387Z"
    },
    "papermill": {
     "duration": 0.024212,
     "end_time": "2025-07-26T13:27:08.085578",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.061366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Class names: ['label0', 'label1', 'label2']\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config = load_data_config(\"/kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/data.yaml\")\n",
    "num_classes = config['nc']\n",
    "class_names = config['names']\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5d6524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.142810Z",
     "iopub.status.busy": "2025-07-26T13:27:08.142555Z",
     "iopub.status.idle": "2025-07-26T13:27:08.833590Z",
     "shell.execute_reply": "2025-07-26T13:27:08.832742Z"
    },
    "papermill": {
     "duration": 0.743211,
     "end_time": "2025-07-26T13:27:08.834806",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.091595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Directory Structure Check for /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch ---\n",
      "Directory /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/train/images: Exists, Contains 6930 files\n",
      "  Example files: ['volume_66_slice_107_jpg.rf.3323c23ffb4eccdd843337b27556bd61.jpg', 'volume_248_slice_71_jpg.rf.24606413467e216b0ef9c1bae9e52280.jpg', 'volume_342_slice_69_jpg.rf.fd98a1f55f0e5a10ccf73e39c48be04b.jpg']\n",
      "Directory /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/train/labels: Exists, Contains 6930 files\n",
      "  Example files: ['volume_355_slice_90_jpg.rf.3329d69004d7cfcde34e54498d57c092.txt', 'volume_27_slice_32_jpg.rf.f8a5cbf01d0b04abae74dfa30fb00f98.txt', 'volume_172_slice_102_jpg.rf.1e354849bb2dac58094a1d147adf833a.txt']\n",
      "Directory /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/valid/images: Exists, Contains 1980 files\n",
      "  Example files: ['volume_362_slice_52_jpg.rf.ae6c7adf7d668c17773b1062e68ec1df.jpg', 'volume_88_slice_59_jpg.rf.6292c6676cdb6ab1f22d0d250d129ffd.jpg', 'volume_369_slice_111_jpg.rf.f32c2ea82bcb6b6a313f116b13d0816d.jpg']\n",
      "Directory /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/valid/labels: Exists, Contains 1980 files\n",
      "  Example files: ['volume_1_slice_48_jpg.rf.44235e70bacd356d0fc58ed80e8e54df.txt', 'volume_330_slice_44_jpg.rf.d5a21bf21e9e9177fc06538155433376.txt', 'volume_254_slice_98_jpg.rf.4343d3b6f6b5a56c514a958baa16418e.txt']\n",
      "\n",
      "Matching check:\n",
      "  Image: /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/train/images/volume_66_slice_107_jpg.rf.3323c23ffb4eccdd843337b27556bd61.jpg\n",
      "  Mask: /kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/train/labels/volume_66_slice_107_jpg.rf.3323c23ffb4eccdd843337b27556bd61.txt\n",
      "\n",
      "Mask content (first 200 chars):\n",
      "0 0.5708333333333333 0.43333333333333335 0.016666666666666666 0.025\n",
      "1 0.54375 0.37083333333333335 0.22083333333333333 0.21666666666666667\n",
      "2 0.5729166666666666 0.43333333333333335 0.0375 0.041666666666\n"
     ]
    }
   ],
   "source": [
    "# Setup paths relative to yaml location\n",
    "base_dir = os.path.dirname(\"/kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch/data.yaml\")\n",
    "train_img_dir = os.path.join(base_dir, \"train/images\")\n",
    "train_mask_dir = os.path.join(base_dir, \"train/labels\")\n",
    "val_img_dir = os.path.join(base_dir, \"valid/images\")\n",
    "val_mask_dir = os.path.join(base_dir, \"valid/labels\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "check_directory_structure(\"/kaggle/input/brain-tumor-dataset/brain tumor.v2-release.yolov7pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a49fcb62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.848008Z",
     "iopub.status.busy": "2025-07-26T13:27:08.847492Z",
     "iopub.status.idle": "2025-07-26T13:27:08.855192Z",
     "shell.execute_reply": "2025-07-26T13:27:08.854690Z"
    },
    "papermill": {
     "duration": 0.015117,
     "end_time": "2025-07-26T13:27:08.856274",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.841157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transforms with smaller image size\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),  # Reduced from 384\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cfb04fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.868732Z",
     "iopub.status.busy": "2025-07-26T13:27:08.868538Z",
     "iopub.status.idle": "2025-07-26T13:27:08.873283Z",
     "shell.execute_reply": "2025-07-26T13:27:08.872787Z"
    },
    "papermill": {
     "duration": 0.012168,
     "end_time": "2025-07-26T13:27:08.874390",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.862222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),  # Reduced from 384\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59468e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:08.887597Z",
     "iopub.status.busy": "2025-07-26T13:27:08.887346Z",
     "iopub.status.idle": "2025-07-26T13:27:20.972762Z",
     "shell.execute_reply": "2025-07-26T13:27:20.971996Z"
    },
    "papermill": {
     "duration": 12.093477,
     "end_time": "2025-07-26T13:27:20.974029",
     "exception": false,
     "start_time": "2025-07-26T13:27:08.880552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6930 valid image-mask pairs out of 6930 images\n"
     ]
    }
   ],
   "source": [
    "# Datasets and loaders\n",
    "train_dataset = TumorSegmentationDataset(\n",
    "    train_img_dir, \n",
    "    train_mask_dir, \n",
    "    transform=train_transform,\n",
    "    num_classes=num_classes  # Now using 3 classes from yaml\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5d8538d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:20.987256Z",
     "iopub.status.busy": "2025-07-26T13:27:20.987025Z",
     "iopub.status.idle": "2025-07-26T13:27:24.239612Z",
     "shell.execute_reply": "2025-07-26T13:27:24.238818Z"
    },
    "papermill": {
     "duration": 3.260466,
     "end_time": "2025-07-26T13:27:24.240761",
     "exception": false,
     "start_time": "2025-07-26T13:27:20.980295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1980 valid image-mask pairs out of 1980 images\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TumorSegmentationDataset(\n",
    "    val_img_dir, \n",
    "    val_mask_dir, \n",
    "    transform=val_transform,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "655525d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:27:24.254089Z",
     "iopub.status.busy": "2025-07-26T13:27:24.253845Z",
     "iopub.status.idle": "2025-07-26T13:29:02.407230Z",
     "shell.execute_reply": "2025-07-26T13:29:02.406381Z"
    },
    "papermill": {
     "duration": 98.167302,
     "end_time": "2025-07-26T13:29:02.414513",
     "exception": false,
     "start_time": "2025-07-26T13:27:24.247211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating class indices...\n",
      "Max class index in train: 2\n",
      "Max class index in val: 2\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Add validation before training\n",
    "print(\"\\nValidating class indices...\")\n",
    "max_train = max(mask.max() for _, mask in train_dataset)\n",
    "max_val = max(mask.max() for _, mask in val_dataset)\n",
    "print(f\"Max class index in train: {max_train}\")\n",
    "print(f\"Max class index in val: {max_val}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bb9d6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:02.427738Z",
     "iopub.status.busy": "2025-07-26T13:29:02.427499Z",
     "iopub.status.idle": "2025-07-26T13:29:02.431352Z",
     "shell.execute_reply": "2025-07-26T13:29:02.430822Z"
    },
    "papermill": {
     "duration": 0.011632,
     "end_time": "2025-07-26T13:29:02.432404",
     "exception": false,
     "start_time": "2025-07-26T13:29:02.420772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert max_train < num_classes, f\"Invalid class index in train set: {max_train} >= {num_classes}\"\n",
    "assert max_val < num_classes, f\"Invalid class index in val set: {max_val} >= {num_classes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2779b1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:02.444759Z",
     "iopub.status.busy": "2025-07-26T13:29:02.444554Z",
     "iopub.status.idle": "2025-07-26T13:29:56.093376Z",
     "shell.execute_reply": "2025-07-26T13:29:56.092426Z"
    },
    "papermill": {
     "duration": 53.656411,
     "end_time": "2025-07-26T13:29:56.094668",
     "exception": false,
     "start_time": "2025-07-26T13:29:02.438257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating mask values...\n",
      "train dataset unique classes: [0, 1, 2]\n",
      "val dataset unique classes: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Validate class indices\n",
    "print(\"\\nValidating mask values...\")\n",
    "for dataset, name in [(train_dataset, 'train'), (val_dataset, 'val')]:\n",
    "    unique_classes = set()\n",
    "    for _, mask in dataset:\n",
    "        unique_classes.update(mask.unique().tolist())\n",
    "    print(f\"{name} dataset unique classes: {sorted(unique_classes)}\")\n",
    "    assert max(unique_classes) < num_classes, \\\n",
    "        f\"Invalid class index in {name} set: {max(unique_classes)} >= {num_classes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aac73644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:56.107923Z",
     "iopub.status.busy": "2025-07-26T13:29:56.107704Z",
     "iopub.status.idle": "2025-07-26T13:29:56.118201Z",
     "shell.execute_reply": "2025-07-26T13:29:56.117359Z"
    },
    "papermill": {
     "duration": 0.018151,
     "end_time": "2025-07-26T13:29:56.119281",
     "exception": false,
     "start_time": "2025-07-26T13:29:56.101130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "--------------------------------------------------\n",
      "Images in train directory: 6930\n",
      "Masks in train directory: 6930\n",
      "\n",
      "Sample image-mask pairs:\n",
      "--------------------------------------------------\n",
      "First 5 train pairs: [('volume_66_slice_107_jpg.rf.3323c23ffb4eccdd843337b27556bd61.jpg', 'volume_66_slice_107_jpg.rf.3323c23ffb4eccdd843337b27556bd61.txt'), ('volume_248_slice_71_jpg.rf.24606413467e216b0ef9c1bae9e52280.jpg', 'volume_248_slice_71_jpg.rf.24606413467e216b0ef9c1bae9e52280.txt'), ('volume_342_slice_69_jpg.rf.fd98a1f55f0e5a10ccf73e39c48be04b.jpg', 'volume_342_slice_69_jpg.rf.fd98a1f55f0e5a10ccf73e39c48be04b.txt'), ('volume_175_slice_75_jpg.rf.e8283a8e19d28d1a3bc5f8632a829255.jpg', 'volume_175_slice_75_jpg.rf.e8283a8e19d28d1a3bc5f8632a829255.txt'), ('volume_85_slice_107_jpg.rf.c6cb235444db622026deff749844f043.jpg', 'volume_85_slice_107_jpg.rf.c6cb235444db622026deff749844f043.txt')]\n",
      "\n",
      "Total valid pairs:\n",
      "--------------------------------------------------\n",
      "Train dataset: 6930 pairs\n",
      "Val dataset: 1980 pairs\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Images in train directory: {len(os.listdir(train_img_dir))}\")\n",
    "print(f\"Masks in train directory: {len(os.listdir(train_mask_dir))}\")\n",
    "print(\"\\nSample image-mask pairs:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"First 5 train pairs:\", train_dataset.valid_pairs[:5])\n",
    "print(f\"\\nTotal valid pairs:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Train dataset: {len(train_dataset)} pairs\")\n",
    "print(f\"Val dataset: {len(val_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b26f446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:56.131941Z",
     "iopub.status.busy": "2025-07-26T13:29:56.131726Z",
     "iopub.status.idle": "2025-07-26T13:29:56.670362Z",
     "shell.execute_reply": "2025-07-26T13:29:56.669577Z"
    },
    "papermill": {
     "duration": 0.546452,
     "end_time": "2025-07-26T13:29:56.671801",
     "exception": false,
     "start_time": "2025-07-26T13:29:56.125349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model, loss, optimizer\n",
    "model = BEiTSegmentation(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68112fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:56.685318Z",
     "iopub.status.busy": "2025-07-26T13:29:56.684900Z",
     "iopub.status.idle": "2025-07-26T13:29:56.689153Z",
     "shell.execute_reply": "2025-07-26T13:29:56.688460Z"
    },
    "papermill": {
     "duration": 0.012147,
     "end_time": "2025-07-26T13:29:56.690335",
     "exception": false,
     "start_time": "2025-07-26T13:29:56.678188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data loaders with smaller batch size\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1,       # Reduced from 2\n",
    "    shuffle=True, \n",
    "    num_workers=0,      # Reduced from 2\n",
    "    pin_memory=True     # Add pin_memory\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1,       # Reduced from 2\n",
    "    shuffle=True, \n",
    "    num_workers=0,      # Reduced from 2\n",
    "    pin_memory=True     # Add pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc8fdbb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:56.703256Z",
     "iopub.status.busy": "2025-07-26T13:29:56.702748Z",
     "iopub.status.idle": "2025-07-26T13:29:56.779482Z",
     "shell.execute_reply": "2025-07-26T13:29:56.778985Z"
    },
    "papermill": {
     "duration": 0.084303,
     "end_time": "2025-07-26T13:29:56.780621",
     "exception": false,
     "start_time": "2025-07-26T13:29:56.696318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get list of test images\n",
    "test_img_dir = os.path.join(base_dir, \"test/images\")\n",
    "test_images = [os.path.join(test_img_dir, f) for f in os.listdir(test_img_dir) \n",
    "              if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "if not test_images:\n",
    "    print(\"Warning: No test images found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f43193",
   "metadata": {
    "papermill": {
     "duration": 0.005717,
     "end_time": "2025-07-26T13:29:56.792365",
     "exception": false,
     "start_time": "2025-07-26T13:29:56.786648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a30607c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:29:56.804996Z",
     "iopub.status.busy": "2025-07-26T13:29:56.804580Z",
     "iopub.status.idle": "2025-07-26T22:17:50.349755Z",
     "shell.execute_reply": "2025-07-26T22:17:50.349074Z"
    },
    "papermill": {
     "duration": 31673.552754,
     "end_time": "2025-07-26T22:17:50.350907",
     "exception": false,
     "start_time": "2025-07-26T13:29:56.798153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:52<00:00, 16.79it/s, loss=0.132]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.10it/s, loss=0.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1452 | Val Loss: 0.1959 | mIoU: 0.4267\n",
      "IoU per class: [0.9399134095177615, 0.18364479923855997, 0.1566250466033696]\n",
      "\n",
      "Generating prediction visualization for: volume_10_slice_91_jpg.rf.b4a84f6c1b5ffcd12fc80f3b99c496f6.jpg\n",
      "No tumor regions detected in volume_10_slice_91_jpg.rf.b4a84f6c1b5ffcd12fc80f3b99c496f6.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 2/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:57<00:00, 16.59it/s, loss=0.0988]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.66it/s, loss=0.0621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0957 | Val Loss: 0.1688 | mIoU: 0.4474\n",
      "IoU per class: [0.9587347179656114, 0.16876809755029581, 0.21461161820073113]\n",
      "\n",
      "Generating prediction visualization for: volume_344_slice_77_jpg.rf.d29c3f0fd784a1365c83e3964539a1aa.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 3/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:56<00:00, 16.64it/s, loss=0.117]\n",
      "Validating: 100%|| 1980/1980 [01:02<00:00, 31.76it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0852 | Val Loss: 0.1759 | mIoU: 0.4324\n",
      "IoU per class: [0.9554102958227654, 0.18154132443091128, 0.16026643419271128]\n",
      "\n",
      "Generating prediction visualization for: volume_22_slice_65_jpg.rf.a6a53575c758dd7714af4a0067c0afa0.jpg\n",
      "No tumor regions detected in volume_22_slice_65_jpg.rf.a6a53575c758dd7714af4a0067c0afa0.jpg\n",
      "\n",
      "Epoch 4/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:13<00:00, 15.99it/s, loss=0.0238]\n",
      "Validating: 100%|| 1980/1980 [01:02<00:00, 31.86it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0768 | Val Loss: 0.1703 | mIoU: 0.4793\n",
      "IoU per class: [0.9489490047791549, 0.22760683102279408, 0.2613652533899511]\n",
      "\n",
      "Generating prediction visualization for: volume_232_slice_53_jpg.rf.4019eb84765417c6eb316fa53661c73d.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 5/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:06<00:00, 16.25it/s, loss=0.128]\n",
      "Validating: 100%|| 1980/1980 [01:00<00:00, 32.66it/s, loss=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0702 | Val Loss: 0.1751 | mIoU: 0.4628\n",
      "IoU per class: [0.9507194812807694, 0.2121946998390023, 0.2255873833491478]\n",
      "\n",
      "Generating prediction visualization for: volume_332_slice_119_jpg.rf.1bd2ebe46fda6948f7c1ea535c7e1ab0.jpg\n",
      "\n",
      "Epoch 6/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:57<00:00, 16.59it/s, loss=0.069]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.92it/s, loss=0.00193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0643 | Val Loss: 0.1749 | mIoU: 0.4795\n",
      "IoU per class: [0.9536793454246979, 0.22359570036722742, 0.2612502989324474]\n",
      "\n",
      "Generating prediction visualization for: volume_65_slice_76_jpg.rf.3653d620f36463b0c7d54b3f0de1f757.jpg\n",
      "No tumor regions detected in volume_65_slice_76_jpg.rf.3653d620f36463b0c7d54b3f0de1f757.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 7/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:56<00:00, 16.63it/s, loss=0.0159]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.11it/s, loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0590 | Val Loss: 0.2398 | mIoU: 0.4702\n",
      "IoU per class: [0.9295405929843517, 0.24460914429121153, 0.23653638339614466]\n",
      "\n",
      "Generating prediction visualization for: volume_55_slice_48_jpg.rf.98698d1d22b717ad3fc6027d6727968c.jpg\n",
      "\n",
      "Epoch 8/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:02<00:00, 16.42it/s, loss=0.0332]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.84it/s, loss=0.0266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0550 | Val Loss: 0.1450 | mIoU: 0.5266\n",
      "IoU per class: [0.9626402911927073, 0.31016258855003714, 0.30707692367530315]\n",
      "\n",
      "Generating prediction visualization for: volume_16_slice_58_jpg.rf.d22c37f0d5f5cd5474e7bb6ff8c4083b.jpg\n",
      "No tumor regions detected in volume_16_slice_58_jpg.rf.d22c37f0d5f5cd5474e7bb6ff8c4083b.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:05<00:00, 16.28it/s, loss=0.127]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.30it/s, loss=0.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0510 | Val Loss: 0.1492 | mIoU: 0.5175\n",
      "IoU per class: [0.959532690418016, 0.2683489878575461, 0.32448533184741607]\n",
      "\n",
      "Generating prediction visualization for: volume_74_slice_66_jpg.rf.dc8c7e96e6ad87234a51ff555fa4889b.jpg\n",
      "No tumor regions detected in volume_74_slice_66_jpg.rf.dc8c7e96e6ad87234a51ff555fa4889b.jpg\n",
      "\n",
      "Epoch 10/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:02<00:00, 16.42it/s, loss=0.0578]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.95it/s, loss=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0476 | Val Loss: 0.1693 | mIoU: 0.5201\n",
      "IoU per class: [0.9575290995750595, 0.30196215038597995, 0.30070500859628513]\n",
      "\n",
      "Generating prediction visualization for: volume_42_slice_97_jpg.rf.7a3553a2ba6cb9f751abecd8abc7d4c0.jpg\n",
      "\n",
      "Epoch 11/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:57<00:00, 16.59it/s, loss=0.0223]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.86it/s, loss=0.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0449 | Val Loss: 0.2256 | mIoU: 0.4980\n",
      "IoU per class: [0.9352986596650512, 0.24858047705647965, 0.3101262351712147]\n",
      "\n",
      "Generating prediction visualization for: volume_147_slice_45_jpg.rf.995948f81d81208bca5765c5496eb42e.jpg\n",
      "No tumor regions detected in volume_147_slice_45_jpg.rf.995948f81d81208bca5765c5496eb42e.jpg\n",
      "\n",
      "Epoch 12/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:56<00:00, 16.63it/s, loss=0.0379]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 34.09it/s, loss=0.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0423 | Val Loss: 0.1628 | mIoU: 0.5373\n",
      "IoU per class: [0.9596034051625666, 0.31479177252907536, 0.3376197940527813]\n",
      "\n",
      "Generating prediction visualization for: volume_72_slice_109_jpg.rf.49fa785338b21bce4a94474a0aea4c09.jpg\n",
      "No tumor regions detected in volume_72_slice_109_jpg.rf.49fa785338b21bce4a94474a0aea4c09.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 13/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:57<00:00, 16.60it/s, loss=0.0354]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.32it/s, loss=0.0818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0400 | Val Loss: 0.1288 | mIoU: 0.5620\n",
      "IoU per class: [0.9673485937228343, 0.3312189970226921, 0.38748455949402877]\n",
      "\n",
      "Generating prediction visualization for: volume_96_slice_92_jpg.rf.280de52f4e37c0f03931e4b4f7d1a0bd.jpg\n",
      "No tumor regions detected in volume_96_slice_92_jpg.rf.280de52f4e37c0f03931e4b4f7d1a0bd.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 14/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:58<00:00, 16.57it/s, loss=0.0398]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.93it/s, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0384 | Val Loss: 0.1313 | mIoU: 0.5753\n",
      "IoU per class: [0.9704579965864769, 0.3380404230140819, 0.41725471460575503]\n",
      "\n",
      "Generating prediction visualization for: volume_287_slice_120_jpg.rf.c9247199695eb415ca99b504971bcc17.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 15/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.50it/s, loss=0.028]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 34.07it/s, loss=0.0996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0365 | Val Loss: 0.1374 | mIoU: 0.5672\n",
      "IoU per class: [0.9686720581155986, 0.36033888092007965, 0.3725167960180272]\n",
      "\n",
      "Generating prediction visualization for: volume_316_slice_109_jpg.rf.6fee35b39f016d495c4ebc3d51b9683c.jpg\n",
      "\n",
      "Epoch 16/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.46it/s, loss=0.06]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.48it/s, loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0352 | Val Loss: 0.1589 | mIoU: 0.5463\n",
      "IoU per class: [0.9618445929168593, 0.32503102685396695, 0.35208083639423327]\n",
      "\n",
      "Generating prediction visualization for: volume_203_slice_81_jpg.rf.f215c0b1cbfbf005fa3cc2e387c1af73.jpg\n",
      "\n",
      "Epoch 17/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:55<00:00, 16.69it/s, loss=0.0383]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.52it/s, loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0338 | Val Loss: 0.1379 | mIoU: 0.5628\n",
      "IoU per class: [0.9636545405051851, 0.3422671473261967, 0.38258802779292583]\n",
      "\n",
      "Generating prediction visualization for: volume_77_slice_80_jpg.rf.e432285c7e9ddacd835a31f3ad7410db.jpg\n",
      "No tumor regions detected in volume_77_slice_80_jpg.rf.e432285c7e9ddacd835a31f3ad7410db.jpg\n",
      "\n",
      "Epoch 18/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.50it/s, loss=0.0287]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.49it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0328 | Val Loss: 0.1401 | mIoU: 0.5667\n",
      "IoU per class: [0.9652370798964756, 0.32559582069354454, 0.40936780313718196]\n",
      "\n",
      "Generating prediction visualization for: volume_166_slice_111_jpg.rf.899afcd06c4b6e1a985895c20d9e4540.jpg\n",
      "\n",
      "Epoch 19/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:54<00:00, 16.74it/s, loss=0.0338]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.24it/s, loss=0.0049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0315 | Val Loss: 0.1341 | mIoU: 0.5807\n",
      "IoU per class: [0.9691142044028959, 0.3736773588035665, 0.39924394694063037]\n",
      "\n",
      "Generating prediction visualization for: volume_228_slice_87_jpg.rf.d2159e49ebfa76a3473eb5cfaca2bb4e.jpg\n",
      "No tumor regions detected in volume_228_slice_87_jpg.rf.d2159e49ebfa76a3473eb5cfaca2bb4e.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 20/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:53<00:00, 16.74it/s, loss=0.0468]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.64it/s, loss=0.0717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0307 | Val Loss: 0.1462 | mIoU: 0.5760\n",
      "IoU per class: [0.9684358137730507, 0.34256004031750675, 0.4170196601384664]\n",
      "\n",
      "Generating prediction visualization for: volume_271_slice_46_jpg.rf.551a6e08558db71e2b751159d1bde1b2.jpg\n",
      "No tumor regions detected in volume_271_slice_46_jpg.rf.551a6e08558db71e2b751159d1bde1b2.jpg\n",
      "\n",
      "Epoch 21/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:53<00:00, 16.75it/s, loss=0.013]\n",
      "Validating: 100%|| 1980/1980 [01:01<00:00, 32.02it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0296 | Val Loss: 0.1391 | mIoU: 0.5736\n",
      "IoU per class: [0.9718165974925264, 0.31810524007994945, 0.4308419615029941]\n",
      "\n",
      "Generating prediction visualization for: volume_341_slice_73_jpg.rf.ff763c2916479073029218667eca87af.jpg\n",
      "No tumor regions detected in volume_341_slice_73_jpg.rf.ff763c2916479073029218667eca87af.jpg\n",
      "\n",
      "Epoch 22/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:18<00:00, 15.81it/s, loss=0.0339]\n",
      "Validating: 100%|| 1980/1980 [01:03<00:00, 31.05it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0290 | Val Loss: 0.1308 | mIoU: 0.6009\n",
      "IoU per class: [0.9724068967879482, 0.3908054369665045, 0.4395099513212951]\n",
      "\n",
      "Generating prediction visualization for: volume_207_slice_68_jpg.rf.2279f87e28730f4779668516cd2088c3.jpg\n",
      "No tumor regions detected in volume_207_slice_68_jpg.rf.2279f87e28730f4779668516cd2088c3.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 23/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:29<00:00, 15.43it/s, loss=0.031]\n",
      "Validating: 100%|| 1980/1980 [01:02<00:00, 31.43it/s, loss=0.0571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0283 | Val Loss: 0.1577 | mIoU: 0.5774\n",
      "IoU per class: [0.9607018362364952, 0.34156319153629056, 0.42992732244369286]\n",
      "\n",
      "Generating prediction visualization for: volume_358_slice_40_jpg.rf.6d8ca5682ffff7ade2e7e1a5631a8308.jpg\n",
      "\n",
      "Epoch 24/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:16<00:00, 15.87it/s, loss=0.0288]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.71it/s, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0276 | Val Loss: 0.1461 | mIoU: 0.5703\n",
      "IoU per class: [0.9666067647713044, 0.3600930076723403, 0.3843257529699703]\n",
      "\n",
      "Generating prediction visualization for: volume_89_slice_37_jpg.rf.1ff414a147a09e3fd6763c8550155221.jpg\n",
      "\n",
      "Epoch 25/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:03<00:00, 16.37it/s, loss=0.026]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.44it/s, loss=0.0988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0273 | Val Loss: 0.1309 | mIoU: 0.5966\n",
      "IoU per class: [0.971488735030385, 0.4004088696084802, 0.41784083718504667]\n",
      "\n",
      "Generating prediction visualization for: volume_203_slice_36_jpg.rf.9d1e4c29df3087c3c5802a7ef67310a5.jpg\n",
      "\n",
      "Epoch 26/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:56<00:00, 16.64it/s, loss=0.0177]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.44it/s, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0263 | Val Loss: 0.1417 | mIoU: 0.5834\n",
      "IoU per class: [0.9700152683929057, 0.3436072456223716, 0.436574090576848]\n",
      "\n",
      "Generating prediction visualization for: volume_72_slice_68_jpg.rf.e3d949488a2c6d1bd09e9187cdafbd75.jpg\n",
      "\n",
      "Epoch 27/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:56<00:00, 16.66it/s, loss=0.016]\n",
      "Validating: 100%|| 1980/1980 [01:00<00:00, 32.86it/s, loss=0.0784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0258 | Val Loss: 0.1331 | mIoU: 0.6008\n",
      "IoU per class: [0.9718380614933365, 0.40777397910198554, 0.42292054710602395]\n",
      "\n",
      "Generating prediction visualization for: volume_83_slice_81_jpg.rf.6612d3ec7400d9147c86583c9b7cc045.jpg\n",
      "\n",
      "Epoch 28/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.47it/s, loss=0.0181]\n",
      "Validating: 100%|| 1980/1980 [01:01<00:00, 32.04it/s, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0255 | Val Loss: 0.1283 | mIoU: 0.6137\n",
      "IoU per class: [0.9699284487751239, 0.4040307154045412, 0.4672516947211853]\n",
      "\n",
      "Generating prediction visualization for: volume_212_slice_88_jpg.rf.5ce7c66b521036388ee22b843cfc1441.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 29/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:53<00:00, 16.76it/s, loss=0.0211]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 35.22it/s, loss=0.0989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0251 | Val Loss: 0.1528 | mIoU: 0.5989\n",
      "IoU per class: [0.9655102486616861, 0.399691851343272, 0.4314888795366434]\n",
      "\n",
      "Generating prediction visualization for: volume_261_slice_72_jpg.rf.7cc98d917e3ab811b116ca636114cd79.jpg\n",
      "\n",
      "Epoch 30/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:49<00:00, 16.93it/s, loss=0.0188]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.66it/s, loss=0.0958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0246 | Val Loss: 0.1247 | mIoU: 0.6189\n",
      "IoU per class: [0.9734947444564384, 0.42285423169738723, 0.46037597991877216]\n",
      "\n",
      "Generating prediction visualization for: volume_150_slice_41_jpg.rf.979930bdca9ed3c5897ff581a8a4d168.jpg\n",
      "No tumor regions detected in volume_150_slice_41_jpg.rf.979930bdca9ed3c5897ff581a8a4d168.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 31/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:49<00:00, 16.91it/s, loss=0.00666]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.61it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0242 | Val Loss: 0.1337 | mIoU: 0.6075\n",
      "IoU per class: [0.972610183545047, 0.3778421065174981, 0.47204489179397857]\n",
      "\n",
      "Generating prediction visualization for: volume_71_slice_70_jpg.rf.4fff6e23b8dd655d6278c23e68db8590.jpg\n",
      "\n",
      "Epoch 32/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:59<00:00, 16.54it/s, loss=0.0105]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 34.88it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0236 | Val Loss: 0.1719 | mIoU: 0.5576\n",
      "IoU per class: [0.9551799699927627, 0.3189095204610643, 0.3988215254061471]\n",
      "\n",
      "Generating prediction visualization for: volume_117_slice_66_jpg.rf.76ef536964091644bcd0d631defdd5a3.jpg\n",
      "\n",
      "Epoch 33/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:58<00:00, 16.56it/s, loss=0.0162]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.12it/s, loss=0.099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0231 | Val Loss: 0.1482 | mIoU: 0.6070\n",
      "IoU per class: [0.9727254353443218, 0.4047417813276662, 0.443386121842191]\n",
      "\n",
      "Generating prediction visualization for: volume_253_slice_41_jpg.rf.c1144253b3302a33b49d58b4ae8388cd.jpg\n",
      "\n",
      "Epoch 34/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:54<00:00, 16.70it/s, loss=0.0175]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.28it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0228 | Val Loss: 0.1536 | mIoU: 0.5686\n",
      "IoU per class: [0.958700304234994, 0.385609152589326, 0.36138002376674183]\n",
      "\n",
      "Generating prediction visualization for: volume_96_slice_62_jpg.rf.30dfe78da7beb1a21aa810b24d04fc48.jpg\n",
      "No tumor regions detected in volume_96_slice_62_jpg.rf.30dfe78da7beb1a21aa810b24d04fc48.jpg\n",
      "\n",
      "Epoch 35/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:56<00:00, 16.62it/s, loss=0.0252]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 35.02it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0226 | Val Loss: 0.1430 | mIoU: 0.6171\n",
      "IoU per class: [0.9710418191816177, 0.43280393725309635, 0.44733533970639794]\n",
      "\n",
      "Generating prediction visualization for: volume_263_slice_127_jpg.rf.9f4085978c4961501677c5fb0d31ece4.jpg\n",
      "\n",
      "Epoch 36/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:50<00:00, 16.89it/s, loss=0.0251]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.45it/s, loss=0.0382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0220 | Val Loss: 0.1249 | mIoU: 0.6296\n",
      "IoU per class: [0.9749405822495781, 0.4238911116556932, 0.48988729111323537]\n",
      "\n",
      "Generating prediction visualization for: volume_328_slice_34_jpg.rf.1a32d71fdcabd9e285eec4f8abd90d7d.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 37/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:59<00:00, 16.53it/s, loss=0.0445]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 35.10it/s, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0220 | Val Loss: 0.1281 | mIoU: 0.6135\n",
      "IoU per class: [0.9737776972381899, 0.4246231641983977, 0.44205729355654316]\n",
      "\n",
      "Generating prediction visualization for: volume_310_slice_74_jpg.rf.1996d6d1706a2da01e92ab0dc84d99ec.jpg\n",
      "\n",
      "Epoch 38/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:48<00:00, 16.98it/s, loss=0.0219]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 34.82it/s, loss=0.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0215 | Val Loss: 0.2140 | mIoU: 0.5596\n",
      "IoU per class: [0.940334117985799, 0.2877404628733481, 0.450586976989624]\n",
      "\n",
      "Generating prediction visualization for: volume_24_slice_41_jpg.rf.f701ceccbfbcc4bc0ec8fbe30a4f1e6d.jpg\n",
      "\n",
      "Epoch 39/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:47<00:00, 17.01it/s, loss=0.0443]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 34.95it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0211 | Val Loss: 0.1485 | mIoU: 0.6149\n",
      "IoU per class: [0.9728077759789769, 0.3887067860288862, 0.48310441055725406]\n",
      "\n",
      "Generating prediction visualization for: volume_227_slice_47_jpg.rf.8d76183c4bd0ff9e94d51975aee0c29a.jpg\n",
      "No tumor regions detected in volume_227_slice_47_jpg.rf.8d76183c4bd0ff9e94d51975aee0c29a.jpg\n",
      "\n",
      "Epoch 40/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:58<00:00, 16.56it/s, loss=0.022]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 35.10it/s, loss=0.0483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0209 | Val Loss: 0.1341 | mIoU: 0.6279\n",
      "IoU per class: [0.9746233766376813, 0.42355833981627544, 0.48542443026974014]\n",
      "\n",
      "Generating prediction visualization for: volume_210_slice_56_jpg.rf.9fa26db89aab8e422b2b2cd32a2150e6.jpg\n",
      "\n",
      "Epoch 41/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:51<00:00, 16.82it/s, loss=0.021]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.35it/s, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0208 | Val Loss: 0.1236 | mIoU: 0.6366\n",
      "IoU per class: [0.9740266317618571, 0.4418892344482197, 0.49402685998959817]\n",
      "\n",
      "Generating prediction visualization for: volume_191_slice_73_jpg.rf.d7530a0bcd4e2ed1a86018d09f57304d.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 42/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:53<00:00, 16.76it/s, loss=0.0143]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 35.11it/s, loss=0.0677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0206 | Val Loss: 0.1304 | mIoU: 0.6181\n",
      "IoU per class: [0.9738553099513092, 0.41589504017081347, 0.46453196980570016]\n",
      "\n",
      "Generating prediction visualization for: volume_105_slice_46_jpg.rf.6081e3b566708b184502578c1ef87de3.jpg\n",
      "No tumor regions detected in volume_105_slice_46_jpg.rf.6081e3b566708b184502578c1ef87de3.jpg\n",
      "\n",
      "Epoch 43/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:51<00:00, 16.82it/s, loss=0.0159]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.49it/s, loss=0.0799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0201 | Val Loss: 0.1217 | mIoU: 0.6372\n",
      "IoU per class: [0.9741929283926396, 0.4269880619169559, 0.5103472982767039]\n",
      "\n",
      "Generating prediction visualization for: volume_135_slice_67_jpg.rf.db3ba2753d932eca442e9e7735ba42eb.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 44/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:01<00:00, 16.46it/s, loss=0.0371]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.54it/s, loss=0.0454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0199 | Val Loss: 0.1273 | mIoU: 0.6331\n",
      "IoU per class: [0.9735027789812146, 0.4560971301034311, 0.4697359623318363]\n",
      "\n",
      "Generating prediction visualization for: volume_221_slice_39_jpg.rf.923e46aae01027f1a0f1ca6bd825993b.jpg\n",
      "\n",
      "Epoch 45/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.47it/s, loss=0.0139]\n",
      "Validating: 100%|| 1980/1980 [00:58<00:00, 33.73it/s, loss=0.0669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0198 | Val Loss: 0.1809 | mIoU: 0.5842\n",
      "IoU per class: [0.9504193646661419, 0.34625019354331477, 0.45583137556263037]\n",
      "\n",
      "Generating prediction visualization for: volume_137_slice_91_jpg.rf.47c0ae1ad5090c914e55e9b3618392ce.jpg\n",
      "\n",
      "Epoch 46/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:57<00:00, 16.60it/s, loss=0.0206]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.39it/s, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0196 | Val Loss: 0.1240 | mIoU: 0.6391\n",
      "IoU per class: [0.9758444474455135, 0.44256767634950206, 0.49902995546336404]\n",
      "\n",
      "Generating prediction visualization for: volume_17_slice_89_jpg.rf.d4ba80753a8003289c98ff3cf1cd2d13.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 47/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.49it/s, loss=0.0148]\n",
      "Validating: 100%|| 1980/1980 [00:56<00:00, 34.79it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0192 | Val Loss: 0.1460 | mIoU: 0.6159\n",
      "IoU per class: [0.9737589158314451, 0.4338384847983912, 0.4400050239143588]\n",
      "\n",
      "Generating prediction visualization for: volume_141_slice_121_jpg.rf.af66c4f71e9295da1ffbd3bb598e80b9.jpg\n",
      "No tumor regions detected in volume_141_slice_121_jpg.rf.af66c4f71e9295da1ffbd3bb598e80b9.jpg\n",
      "\n",
      "Epoch 48/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:52<00:00, 16.80it/s, loss=0.0114]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.30it/s, loss=0.0712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0192 | Val Loss: 0.1365 | mIoU: 0.6215\n",
      "IoU per class: [0.97176464998151, 0.4242922119021585, 0.4683773304305048]\n",
      "\n",
      "Generating prediction visualization for: volume_289_slice_89_jpg.rf.4ad7daec38802ece7096ac48fb535d19.jpg\n",
      "\n",
      "Epoch 49/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:01<00:00, 16.45it/s, loss=0.0156]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.05it/s, loss=0.0658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0190 | Val Loss: 0.1215 | mIoU: 0.6470\n",
      "IoU per class: [0.9755510587342402, 0.46773242126138476, 0.49785807688712724]\n",
      "\n",
      "Generating prediction visualization for: volume_179_slice_39_jpg.rf.687b2f378ae713d3d0fdc563f181a0a0.jpg\n",
      "Best model saved.\n",
      "\n",
      "Epoch 50/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:00<00:00, 16.48it/s, loss=0.0268]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.19it/s, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0186 | Val Loss: 0.1437 | mIoU: 0.6174\n",
      "IoU per class: [0.9716708399446804, 0.41377876706493183, 0.4668132358238654]\n",
      "\n",
      "Generating prediction visualization for: volume_175_slice_90_jpg.rf.e8f540689766f093f29b54f81e1ba978.jpg\n",
      "\n",
      "Epoch 51/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [06:58<00:00, 16.58it/s, loss=0.0219]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.10it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0184 | Val Loss: 0.1595 | mIoU: 0.6063\n",
      "IoU per class: [0.9640430706578004, 0.39367575021776524, 0.46120674872285716]\n",
      "\n",
      "Generating prediction visualization for: volume_114_slice_93_jpg.rf.7fad3f55a0cd8e1ce4717fcca106d1d0.jpg\n",
      "\n",
      "Epoch 52/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:06<00:00, 16.27it/s, loss=0.0384]\n",
      "Validating: 100%|| 1980/1980 [01:00<00:00, 32.93it/s, loss=0.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0184 | Val Loss: 0.1182 | mIoU: 0.6441\n",
      "IoU per class: [0.9755507877462535, 0.454770665382277, 0.5020184524674469]\n",
      "\n",
      "Generating prediction visualization for: volume_160_slice_62_jpg.rf.8c99ba9e028debb1505e65af183d8935.jpg\n",
      "No tumor regions detected in volume_160_slice_62_jpg.rf.8c99ba9e028debb1505e65af183d8935.jpg\n",
      "\n",
      "Epoch 53/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:03<00:00, 16.36it/s, loss=0.0196]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.28it/s, loss=0.0755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0184 | Val Loss: 0.1389 | mIoU: 0.6237\n",
      "IoU per class: [0.9705559001698063, 0.40945007756611024, 0.49110016544458923]\n",
      "\n",
      "Generating prediction visualization for: volume_150_slice_101_jpg.rf.8fe63c380e6e4214f4727e12e46c4d40.jpg\n",
      "\n",
      "Epoch 54/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:05<00:00, 16.28it/s, loss=0.0181]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.53it/s, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0179 | Val Loss: 0.1430 | mIoU: 0.6320\n",
      "IoU per class: [0.9730382889219652, 0.45130537037801366, 0.4715507225220103]\n",
      "\n",
      "Generating prediction visualization for: volume_342_slice_86_jpg.rf.4201a8eccac81cc9d3dd5760e13d986d.jpg\n",
      "\n",
      "Epoch 55/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:04<00:00, 16.34it/s, loss=0.0159]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.09it/s, loss=0.0422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0178 | Val Loss: 0.1364 | mIoU: 0.6371\n",
      "IoU per class: [0.9742012361832976, 0.4645064538405247, 0.4727343603924118]\n",
      "\n",
      "Generating prediction visualization for: volume_316_slice_109_jpg.rf.6fee35b39f016d495c4ebc3d51b9683c.jpg\n",
      "\n",
      "Epoch 56/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:07<00:00, 16.21it/s, loss=0.0235]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.24it/s, loss=0.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0177 | Val Loss: 0.1336 | mIoU: 0.6439\n",
      "IoU per class: [0.9746499947056788, 0.45464367753756524, 0.5025019557479717]\n",
      "\n",
      "Generating prediction visualization for: volume_137_slice_91_jpg.rf.47c0ae1ad5090c914e55e9b3618392ce.jpg\n",
      "\n",
      "Epoch 57/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:12<00:00, 16.03it/s, loss=0.0126]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.01it/s, loss=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0174 | Val Loss: 0.1272 | mIoU: 0.6439\n",
      "IoU per class: [0.9723158928108989, 0.45030054474188297, 0.5091877340091896]\n",
      "\n",
      "Generating prediction visualization for: volume_274_slice_123_jpg.rf.d8e37dcb8479ce47a20c891a36342a51.jpg\n",
      "\n",
      "Epoch 58/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:12<00:00, 16.04it/s, loss=0.0205]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.22it/s, loss=0.0152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0174 | Val Loss: 0.1358 | mIoU: 0.6349\n",
      "IoU per class: [0.97317472001813, 0.43329741026875923, 0.49818921564096735]\n",
      "\n",
      "Generating prediction visualization for: volume_305_slice_134_jpg.rf.8a9645e5ccfb9a7e8945c1efe8284905.jpg\n",
      "\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:08<00:00, 16.17it/s, loss=0.0319]\n",
      "Validating: 100%|| 1980/1980 [00:57<00:00, 34.53it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0173 | Val Loss: 0.1518 | mIoU: 0.6227\n",
      "IoU per class: [0.9708860793776645, 0.41928676977253077, 0.47797342372928814]\n",
      "\n",
      "Generating prediction visualization for: volume_203_slice_36_jpg.rf.9d1e4c29df3087c3c5802a7ef67310a5.jpg\n",
      "\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 6930/6930 [07:03<00:00, 16.34it/s, loss=0.00917]\n",
      "Validating: 100%|| 1980/1980 [00:59<00:00, 33.42it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0170 | Val Loss: 0.1346 | mIoU: 0.6329\n",
      "IoU per class: [0.9737531107929562, 0.45488732897085066, 0.4700601196464046]\n",
      "\n",
      "Generating prediction visualization for: volume_221_slice_40_jpg.rf.96f99038840825da52449f8039f4a006.jpg\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_miou = 0\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    val_loss, val_miou, val_ious = validate(model, val_loader, criterion, device, num_classes)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | mIoU: {val_miou:.4f}\")\n",
    "    print(f\"IoU per class: {val_ious}\")\n",
    "\n",
    "    # Visualize prediction on random test image\n",
    "    if test_images:\n",
    "        random_test_img = random.choice(test_images)\n",
    "        print(f\"\\nGenerating prediction visualization for: {os.path.basename(random_test_img)}\")\n",
    "        visualize_prediction(model, random_test_img, device, class_names)\n",
    "\n",
    "    # Save best model\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        torch.save(model.state_dict(), \"best_beit_segmentation.pth\")\n",
    "        print(\"Best model saved.\")\n",
    "    \n",
    "    # Clear memory again\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c54621",
   "metadata": {
    "papermill": {
     "duration": 33.548707,
     "end_time": "2025-07-26T22:18:58.082806",
     "exception": false,
     "start_time": "2025-07-26T22:18:24.534099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7242832,
     "sourceId": 11549509,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31961.833468,
   "end_time": "2025-07-26T22:19:34.073796",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-26T13:26:52.240328",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
